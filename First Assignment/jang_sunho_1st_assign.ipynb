{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "Submit the 1st assignment.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sunhojjang/ai-security/blob/master/Submit_the_1st_assignment.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0D1ctZ0nhgc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n0pVZze6C9Zq"
      },
      "source": [
        "\n",
        "Generating Names with a Character-Level RNN\n",
        "*******************************************\n",
        "\n",
        "\n",
        "Preparing the Data\n",
        "==================\n",
        "\n",
        ".. Note::\n",
        "   Download the data from\n",
        "   `here <https://download.pytorch.org/tutorial/data.zip>`_\n",
        "   and extract it to the current directory.\n",
        "\n",
        "See the last tutorial for more detail of this process. In short, there\n",
        "are a bunch of plain text files ``data/names/[Language].txt`` with a\n",
        "name per line. We split lines into an array, convert Unicode to ASCII,\n",
        "and end up with a dictionary ``{language: [names ...]}``.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2gJ3QCtvnhgl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from __future__ import unicode_literals, print_function, division #__future__라는 모듈에서 unicode_literals,print_fuction,division 데이터를 가져온다\n",
        "from io import open #io라는 모듈에서 새로운 파일을 가져온다\n",
        "import glob #glob 모듈 전체를 가져온다.\n",
        "import os #os 모듈 전체를 가져온다.\n",
        "import unicodedata #unicodedata 모듈 전체를 가져온다.\n",
        "import string #string 모듈 전체를 가져온다.\n",
        "\n",
        "all_letters = string.ascii_letters + \" .,;'-\" #all_letters에 string 모듈의 아스키 코드를 적용\n",
        "n_letters = len(all_letters) + 1 # n_letters는 all_letters의 크기를 나타냄\n",
        "\n",
        "#glob함수의 함수값을 반환하는 함수\n",
        "def findFiles(path): return glob.glob(path)\n",
        "\n",
        "# 유니코드 문자열을 아스키 코드로 변환하는 함수\n",
        "def unicodeToAscii(s):\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn'\n",
        "        and c in all_letters\n",
        "    )\n",
        "\n",
        "# 파일을 읽고 라인으로 분리하는 함수\n",
        "def readLines(filename):\n",
        "    lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
        "    return [unicodeToAscii(line) for line in lines]\n",
        "\n",
        "category_lines = {} #딕셔너리 category_lines 생성\n",
        "all_categories = [] #리스트 all_categories 생성\n",
        "for filename in findFiles('data/names/*.txt'):\n",
        "    category = os.path.splitext(os.path.(filename))[0]\n",
        "    all_categories.append(category)\n",
        "    lines = readLines(filename)\n",
        "    category_lines[category] = lines #카테고리의 크기만큼 category_lines에 lines를 저장\n",
        "\n",
        "n_categories = len(all_categories) #n_categories는 all_categories의 크기를 나타냄\n",
        "\n",
        "if n_categories == 0: #n_categories가 0이라면 예외를 발생시킨다\n",
        "    raise RuntimeError('Data not found. Make sure that you downloaded data '\n",
        "        'from https://download.pytorch.org/tutorial/data.zip and extract it to '\n",
        "        'the current directory.')\n",
        "\n",
        "print('# categories:', n_categories, all_categories)\n",
        "print(unicodeToAscii(\"O'Néàl\"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KmeLYaiRnhgo",
        "colab_type": "text"
      },
      "source": [
        "Creating the Network\n",
        "====================\n",
        "\n",
        "This network extends `the last tutorial's RNN <#Creating-the-Network>`__\n",
        "with an extra argument for the category tensor, which is concatenated\n",
        "along with the others. The category tensor is a one-hot vector just like\n",
        "the letter input.\n",
        "\n",
        "We will interpret the output as the probability of the next letter. When\n",
        "sampling, the most likely output letter is used as the next input\n",
        "letter.\n",
        "\n",
        "I added a second linear layer ``o2o`` (after combining hidden and\n",
        "output) to give it more muscle to work with. There's also a dropout\n",
        "layer, which `randomly zeros parts of its\n",
        "input <https://arxiv.org/abs/1207.0580>`__ with a given probability\n",
        "(here 0.1) and is usually used to fuzz inputs to prevent overfitting.\n",
        "Here we're using it towards the end of the network to purposely add some\n",
        "chaos and increase sampling variety.\n",
        "\n",
        ".. figure:: https://i.imgur.com/jzVrf7f.png\n",
        "   :alt:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj1zbKUcnhgp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch #torch 모듈 가져오기\n",
        "import torch.nn as nn #torch.nn 모듈을 가져오면서 이름을 nn으로 지정\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, output_size):\n",
        "        super(RNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "\n",
        "        self.i2h = nn.Linear(n_categories + input_size + hidden_size, hidden_size) #샘플링 할때 가장 확률이 높은 문자가 다음 input이 된다\n",
        "        self.i2o = nn.Linear(n_categories + input_size + hidden_size, output_size) #hidden과 출력값을 결합\n",
        "        self.o2o = nn.Linear(hidden_size + output_size, output_size) #두 번째 선형레이어를 추가\n",
        "        self.dropout = nn.Dropout(0.1) #주어진 확률(0.1)로 입력을 무작위로 만든다\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, category, input, hidden):\n",
        "        input_combined = torch.cat((category, input, hidden), 1)\n",
        "        hidden = self.i2h(input_combined)\n",
        "        output = self.i2o(input_combined)\n",
        "        output_combined = torch.cat((hidden, output), 1) #출력값에 hidden값을 결합\n",
        "        output = self.o2o(output_combined)\n",
        "        output = self.dropout(output)\n",
        "        output = self.softmax(output)\n",
        "        return output, hidden #출력값과 hidden값을 반환\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, self.hidden_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qaRDpzSfnhgs",
        "colab_type": "text"
      },
      "source": [
        "Training\n",
        "=========\n",
        "Preparing for Training\n",
        "----------------------\n",
        "\n",
        "First of all, helper functions to get random pairs of (category, line):\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LNIyvRTynhgt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import random\n",
        "\n",
        "#카테고리와 줄의 쌍을 얻으려는 함수\n",
        "\n",
        "def randomChoice(l):\n",
        "    return l[random.randint(0, len(l) - 1)] # 리스트의 원소를 무작위로 반환\n",
        "\n",
        "def randomTrainingPair():\n",
        "    category = randomChoice(all_categories) # 임의의 카레고리를 얻은 후\n",
        "    line = randomChoice(category_lines[category]) #그 카테고리에서 임의의 줄을 얻기\n",
        "    return category, line"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOjfBeV7nhgx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 카테고리의 원-핫 벡터\n",
        "def categoryTensor(category):\n",
        "    li = all_categories.index(category)\n",
        "    tensor = torch.zeros(1, n_categories)\n",
        "    tensor[0][li] = 1\n",
        "    return tensor\n",
        "\n",
        "# 입력을 위해 처음 문자부터 마지막 문자까지의 원-핫 행렬\n",
        "def inputTensor(line):\n",
        "    tensor = torch.zeros(len(line), 1, n_letters)\n",
        "    for li in range(len(line)):\n",
        "        letter = line[li]\n",
        "        tensor[li][0][all_letters.find(letter)] = 1\n",
        "    return tensor\n",
        "\n",
        "# 타겟을 위해 두 번째 문자부터 마지막 문자까지의 LongTensor\n",
        "def targetTensor(line):\n",
        "    letter_indexes = [all_letters.find(line[li]) for li in range(1, len(line))]\n",
        "    letter_indexes.append(n_letters - 1) # EOS\n",
        "    return torch.LongTensor(letter_indexes)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YFFTB1C1nhg0",
        "colab_type": "text"
      },
      "source": [
        "For convenience during training we'll make a ``randomTrainingExample``\n",
        "function that fetches a random (category, line) pair and turns them into\n",
        "the required (category, input, target) tensors.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U78wmzT-nhg0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 임의의 카테고리, 라인쌍에서 category, input, and target tensors를 만든다\n",
        "def randomTrainingExample():\n",
        "    category, line = randomTrainingPair()\n",
        "    category_tensor = categoryTensor(category)\n",
        "    input_line_tensor = inputTensor(line)\n",
        "    target_line_tensor = targetTensor(line)\n",
        "    return category_tensor, input_line_tensor, target_line_tensor"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M1Fnfg9Inhg3",
        "colab_type": "text"
      },
      "source": [
        "Training the Network\n",
        "--------------------\n",
        "\n",
        "In contrast to classification, where only the last output is used, we\n",
        "are making a prediction at every step, so we are calculating loss at\n",
        "every step.\n",
        "\n",
        "The magic of autograd allows you to simply sum these losses at each step\n",
        "and call backward at the end.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qk0T0oK3nhg4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "criterion = nn.NLLLoss()\n",
        "\n",
        "learning_rate = 0.0005\n",
        "\n",
        "def train(category_tensor, input_line_tensor, target_line_tensor):\n",
        "    target_line_tensor.unsqueeze_(-1)\n",
        "    hidden = rnn.initHidden()\n",
        "\n",
        "    rnn.zero_grad()\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for i in range(input_line_tensor.size(0)):\n",
        "        output, hidden = rnn(category_tensor, input_line_tensor[i], hidden)\n",
        "        l = criterion(output, target_line_tensor[i])\n",
        "        loss += l #손실들을 합한다\n",
        "\n",
        "    loss.backward() #모든 단계에서 손실을 계산해야함\n",
        "\n",
        "    for p in rnn.parameters():\n",
        "        p.data.add_(-learning_rate, p.grad.data)\n",
        "\n",
        "    return output, loss.item() / input_line_tensor.size(0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmLlid3Dnhg7",
        "colab_type": "text"
      },
      "source": [
        "To keep track of how long training takes I am adding a\n",
        "``timeSince(timestamp)`` function which returns a human readable string:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bZsND4tsnhg8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time #time 모듈을 가져온다\n",
        "import math #math 모듈을 가져온다\n",
        "\n",
        "#학습에 걸리는 시간을 확인하려고\n",
        "#문자열을 반환하는 함수\n",
        "def timeSince(since):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g5BywmJLnhhA",
        "colab_type": "text"
      },
      "source": [
        "Training is business as usual - call train a bunch of times and wait a\n",
        "few minutes, printing the current time and loss every ``print_every``\n",
        "examples, and keeping store of an average loss per ``plot_every`` examples\n",
        "in ``all_losses`` for plotting later.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vUEPntqmnhhB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rnn = RNN(n_letters, 128, n_letters)\n",
        "\n",
        "n_iters = 100000\n",
        "print_every = 5000\n",
        "plot_every = 500\n",
        "all_losses = []\n",
        "total_loss = 0 # Reset every plot_every iters\n",
        "\n",
        "start = time.time() #현재 시간과 손실을 출력\n",
        "\n",
        "for iter in range(1, n_iters + 1):\n",
        "    output, loss = train(*randomTrainingExample())\n",
        "    total_loss += loss\n",
        "\n",
        "    if iter % print_every == 0:\n",
        "        print('%s (%d %d%%) %.4f' % (timeSince(start), iter, iter / n_iters * 100, loss))\n",
        "\n",
        "    if iter % plot_every == 0:\n",
        "        all_losses.append(total_loss / plot_every) #평균 손실을 저장\n",
        "        total_loss = 0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U_sb9ElcnhhD",
        "colab_type": "text"
      },
      "source": [
        "Plotting the Losses\n",
        "-------------------\n",
        "\n",
        "Plotting the historical loss from all\\_losses shows the network\n",
        "learning:\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUbDnEonnhhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt #matplotlib.pyplot 모듈을 가져오면서 이름을 plt로 지정\n",
        "import matplotlib.ticker as ticker #matplotlib.ticker모듈을 가져오면서 이름을 ticker로 지정\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(all_losses) #손실의 도식화를 통해 네트워크 학습을 보여줌"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}