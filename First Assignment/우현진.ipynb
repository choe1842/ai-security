{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    },
    "colab": {
      "name": "neural_networks_tutorial.ipynb의 사본",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonjenny/ai-security/blob/master/First%20assignment/%EC%9A%B0%ED%98%84%EC%A7%84.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nXSiVK5_JTY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# nn 패키지를 사용하여 신경망 정의를 한다.\n",
        "# nn 패키지는 모델을 정의하고 미분하는데 autograd를 사용한다.\n",
        "\n",
        "import torch # torch 모듈을 가져온다.\n",
        "import torch.nn as nn # torch.nn을 nn 모듈로 가져온다. \n",
        "import torch.nn.functional as F #torch.nn.functional을 F모듈로 가져온다.\n",
        "\n",
        "#nn.Module은 신경망(Neural Network) 모듈로\n",
        "#파라미터를 GPU로 옮기거나, 전송, 로드 등 파라미터를 캡슐화하는 편리한 방법이다.\n",
        "class Net(nn.Module): #nn.Module 을 통해 Net 클래스를 정의한다.\n",
        "\n",
        "    def __init__(self):#\n",
        "        super(Net, self).__init__()\n",
        "       # 입력 이미지: 1개\n",
        "        # 출력 체널: 6개\n",
        "        # 커널 크기: 5X5\n",
        "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
        "        # 선형 모델: y = Wx + b\n",
        "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "\n",
        "        #x를 매개변수로 forward 함수를 정의해준다.\n",
        "        #forward함수에서는 모든 Tensor연산을 사용할 수 있다.\n",
        "    def forward(self, x):\n",
        "        # (2, 2) 윈도우로 MaxPooling\n",
        "        x = F.max_pool2d(F.relu(self.conv1(x)), (2, 2))\n",
        "        # size가 정사각형인 경우, (2,2) 대신 2로 하나의 숫자를 지정하여 쓰기도 한다.\n",
        "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
        "        # view는 텐서의 크기를 변형하는 함수이다.\n",
        "        x = x.view(-1, self.num_flat_features(x))\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x #x 초기화\n",
        "        \n",
        "        \n",
        "    def num_flat_features(self, x):\n",
        "        size = x.size()[1:]  # 배치 차원을 제외한 모든 차원\n",
        "        num_features = 1\n",
        "        for s in size: # size라는 목록의 각각의 항목 s에 대하여\n",
        "            num_features *= s #num features = num features * s 이다.\n",
        "        return num_features #초기화\n",
        "\n",
        "\n",
        "net = Net()\n",
        "print(net)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eny2e7lW_JTr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#모든 학습가능한 파라미터는 net.parameters()에 저장돼있다.\n",
        "\n",
        "params = list(net.parameters()) #파라미터에 대한 리스트 생성.\n",
        "print(len(params)) #파라미터 목록에 몇 개가 들어있는지를 출력해준다.\n",
        "print(params[0].size())  # conv1의 가중치 크기"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rUthBSPD_JT4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#forward함수의 입력은 autograd.Variable(자동미분변수)이므로 ouput도 autograd.Variable(자동미분변수)이다.\n",
        "#이 네트워크의 예상 input size는 32X32이다. \n",
        "#이 네트워크에  MNIST 데이터셋을 사용하기 위해서는 크기를 32x32로 변경해야 한다.\n",
        "input = torch.randn(1, 1, 32, 32)\n",
        "out = net(input)\n",
        "print(out)\n",
        "# 랜덤함수로 토치를 만들고 net에 통과시켰다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QOOvghec_JUC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#모든 파라미터의 그라디언트 버퍼를 0으로 설정하고\n",
        "net.zero_grad()\n",
        "#랜덤으로 그라디언트를 역전파한다.\n",
        "out.backward(torch.randn(1, 10))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdMzzY-s_JUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#이제부터 nn패키지에 존재하는 여러 손실 함수들 중 nn.MSELoss를 사용한다.\n",
        "output = net(input)\n",
        "target = torch.randn(10)  # 예제로 사용할 더미 타켓\n",
        "target = target.view(1, -1) # 동일한 형태의 더미 타켓\n",
        "#nn.MSELoss는 오차 함수이다.\n",
        "#이 함수는 입력과 타켓의 평균 제곱 오차를 계산한다.\n",
        "criterion = nn.MSELoss() \n",
        "\n",
        "\n",
        "loss = criterion(output, target)\n",
        "print(loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCy-Odes_JUP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(loss.grad_fn)  # 출력함수를 통해 grand텐서 출력 #MSELoss\n",
        "print(loss.grad_fn.next_functions[0][0])  # 출력함수를 통해 grand텐서 출력# Linear\n",
        "print(loss.grad_fn.next_functions[0][0].next_functions[0][0]) # 출력함수를 통해 grand텐서 출력 # ReLU"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6Gi1miL_JUT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net.zero_grad()     # 모든 파라미터의 그라디언트 버퍼를 0으로 한다.\n",
        "\n",
        "print('conv1.bias.grad before backward')\n",
        "print(net.conv1.bias.grad) #출력함수를 통해 conv1 바이어스 그라디언트 값을 출력한다.\n",
        "\n",
        "loss.backward() #loss.backward()를 호출하면 전체 그래프는 손실에 대해 미분이 계산된다.\n",
        "#오차를 역전파하기 위해 loss.backward()를 호출하여 존재하는 그라디언트를 초기화 해야한다.\n",
        "#초기화하지 않으면 그라디언트에 그라디언트가 누적되어 저장된다.\n",
        "\n",
        "\n",
        "print('conv1.bias.grad after backward') #출력함수를 통해 backwardgks conv1의 바이어스 그라디언트 값을 출력한다.\n",
        "print(net.conv1.bias.grad)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FVfKVYy6_JUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.optim as optim # torch.optim 을 optim 모듈로 가져온다. \n",
        "\n",
        "# create your optimizer\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.01)\n",
        "\n",
        "# in your training loop:\n",
        "optimizer.zero_grad()   # 그라디언트 버퍼를 0으로 초기화해준다.\n",
        "output = net(input)\n",
        "loss = criterion(output, target)\n",
        "loss.backward() # loss.backward함수로 그라디언트를 초기화해준다.\n",
        "optimizer.step()    # Does the update\n",
        "\n",
        "#tutorial 출처 :\n",
        "#https://pytorch.org/tutorials/beginner/blitz/neural_networks_tutorial.html#\n",
        "\n",
        "#참고자료:\n",
        "#http://taewan.kim/trans/pytorch/tutorial/blits/03_neural_networks/\n",
        "#https://blog.naver.com/PostView.nhn?blogId=keum_zz6&logNo=221319428565&parentCategoryNo=&categoryNo=15&viewDate=&isShowPopularPosts=true&from=search\n",
        "#https://9bow.github.io/PyTorch-tutorials-kr-0.3.1/beginner/former_torchies/nn_tutorial.html"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}